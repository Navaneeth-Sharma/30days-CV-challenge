{"cells":[{"cell_type":"markdown","metadata":{},"source":["# FRCNN + Selective Search in Pytorch"]},{"cell_type":"markdown","metadata":{},"source":["### Importing Libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:16:45.709885Z","iopub.status.busy":"2022-03-02T14:16:45.709423Z","iopub.status.idle":"2022-03-02T14:17:09.794363Z","shell.execute_reply":"2022-03-02T14:17:09.793228Z","shell.execute_reply.started":"2022-03-02T14:16:45.709847Z"},"trusted":true},"outputs":[],"source":["!pip install -q --upgrade selectivesearch torch_snippets\n","!pip install jsonlines"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:17:09.796920Z","iopub.status.busy":"2022-03-02T14:17:09.796650Z","iopub.status.idle":"2022-03-02T14:17:15.549748Z","shell.execute_reply":"2022-03-02T14:17:15.548751Z","shell.execute_reply.started":"2022-03-02T14:17:09.796891Z"},"trusted":true},"outputs":[],"source":["from torch_snippets import *\n","import selectivesearch\n","from torchvision import transforms, models, datasets\n","from torch_snippets import Report\n","from torchvision.ops import nms\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import os\n","import random\n","import warnings\n","from bs4 import BeautifulSoup\n","import torch.nn as nn\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{},"source":["### Data Prep"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:17:15.552210Z","iopub.status.busy":"2022-03-02T14:17:15.551892Z","iopub.status.idle":"2022-03-02T14:17:16.083483Z","shell.execute_reply":"2022-03-02T14:17:16.082578Z","shell.execute_reply.started":"2022-03-02T14:17:15.552168Z"},"trusted":true},"outputs":[],"source":["images_directory = \"../input/simple-object-detection/datasets/images\"\n","annotations_directory=\"../input/simple-object-detection/datasets/annotations\"\n","\n","#Read Directory\n","\n","img_paths = sorted([os.path.join(fname) for fname in os.listdir(images_directory) if fname.endswith(\".jpg\")])\n","label_paths = sorted([os.path.join(annotations_directory, fname) for fname in os.listdir(annotations_directory) if fname.endswith(\".xml\")])\n","\n","k = len(img_paths)\n","\n","data_list = []\n","\n","#Generate Data Frame \n","\n","for i in range(k):  \n","    annotation_file=label_paths[i]\n","    ds = BeautifulSoup(open(annotation_file).read(), \"html.parser\")\n","\n","    # Iterating over each object elements\n","    for o in ds.find_all(\"object\"):\n","        \n","        x_min = max(0, int(float(o.find(\"xmin\").string)))\n","        y_min = max(0, int(float(o.find(\"ymin\").string)))\n","        x_max = min(int(ds.find(\"width\").string), int(float(o.find(\"xmax\").string)))\n","        y_max = min(int(ds.find(\"height\").string), int(float(o.find(\"ymax\").string)))\n","        \n","        # in case the boundary goes above its limis, providing some restrictions.\n","        if x_min >= x_max or y_min >= y_max:\n","            continue\n","        elif x_max <= x_min or y_max <= y_min:\n","            continue\n","        \n","        sample = [str(img_paths[i]), x_min, y_min, x_max, y_max]\n","        \n","        data_list.append(sample)\n","        \n","data = pd.DataFrame(data_list)\n","\n","# snippet source : HERIBERTO RANGEL ROBLES (Kaggle)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:17:16.085564Z","iopub.status.busy":"2022-03-02T14:17:16.085325Z","iopub.status.idle":"2022-03-02T14:17:16.091989Z","shell.execute_reply":"2022-03-02T14:17:16.090760Z","shell.execute_reply.started":"2022-03-02T14:17:16.085535Z"},"trusted":true},"outputs":[],"source":["IMAGE_ROOT = '../input/simple-object-detection/datasets/images'\n","data = data.rename(columns={\n","    0: 'Image_name',\n","    1:'x1',\n","    2:'y1',\n","    3:'x2',\n","    4:'y2'\n","})"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:17:16.094120Z","iopub.status.busy":"2022-03-02T14:17:16.093465Z","iopub.status.idle":"2022-03-02T14:17:16.118400Z","shell.execute_reply":"2022-03-02T14:17:16.117768Z","shell.execute_reply.started":"2022-03-02T14:17:16.094083Z"},"trusted":true},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:17:16.120902Z","iopub.status.busy":"2022-03-02T14:17:16.119805Z","iopub.status.idle":"2022-03-02T14:17:16.327651Z","shell.execute_reply":"2022-03-02T14:17:16.326556Z","shell.execute_reply.started":"2022-03-02T14:17:16.120851Z"},"trusted":true},"outputs":[],"source":["class OpenImagesData(Dataset):\n","    def __init__(self, df, image_folder=IMAGE_ROOT):\n","        self.root = image_folder\n","        self.df = df\n","        self.unique_images = df['Image_name'].unique()\n","\n","    def __len__(self): return len(self.unique_images)\n","\n","    def __getitem__(self, ix):\n","        image_id = self.unique_images[ix]\n","        image_path = f'{self.root}/{image_id}'\n","        print(image_path)\n","        image = cv2.imread(image_path, 1)[...,::-1] # convert BGR to RGB\n","\n","        h, w, _ = image.shape\n","        df = self.df.copy()\n","        df = df[df['Image_name'] == image_id]\n","        boxes = df['x1,y1,x2,y2'.split(',')].values\n","        classes = ['star']  #here only star is there or u can do with data frame in other situations\n","        return image, boxes, classes, image_path\n","\n","\n","ds = OpenImagesData(df=data)\n","im, bbs, clss, _ = ds[0]\n","show(im, bbs=bbs, texts=clss, sz=5)"]},{"cell_type":"markdown","metadata":{},"source":["### Basic Visualization"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:17:16.329089Z","iopub.status.busy":"2022-03-02T14:17:16.328874Z","iopub.status.idle":"2022-03-02T14:17:17.598966Z","shell.execute_reply":"2022-03-02T14:17:17.597942Z","shell.execute_reply.started":"2022-03-02T14:17:16.329063Z"},"trusted":true},"outputs":[],"source":["def extract_candidates(img):\n","    img_lbl, regions = selectivesearch.selective_search(img, scale=200, min_size=100)\n","    img_area = np.prod(img.shape[:2])\n","    candidates = []\n","    for r in regions:\n","        if r['rect'] in candidates: continue\n","        if r['size'] < (0.05*img_area): continue\n","        if r['size'] > (1*img_area): continue\n","        x, y, w, h = r['rect']\n","        candidates.append(list(r['rect']))\n","    return candidates\n","\n","ds = OpenImagesData(df=data)\n","im, bbs, clss, _ = ds[0]\n","\n","candidates = extract_candidates(im)\n","# candidates\n","plt.figure()\n","for i,candidate in enumerate(candidates):\n","    print(candidate)\n","    show(im, bbs=[candidate], texts=clss, sz=5)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:22:34.203861Z","iopub.status.busy":"2022-03-02T14:22:34.203073Z","iopub.status.idle":"2022-03-02T14:22:34.213118Z","shell.execute_reply":"2022-03-02T14:22:34.212204Z","shell.execute_reply.started":"2022-03-02T14:22:34.203812Z"},"trusted":true},"outputs":[],"source":["def extract_iou(boxA, boxB, epsilon=1e-5):\n","    x1 = max(boxA[0], boxB[0])\n","    y1 = max(boxA[1], boxB[1])\n","    x2 = min(boxA[2], boxB[2])\n","    y2 = min(boxA[3], boxB[3])\n","    width = (x2 - x1)\n","    height = (y2 - y1)\n","    if (width<0) or (height <0):\n","        return 0.0\n","    area_overlap = width * height\n","    area_a = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n","    area_b = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n","    area_combined = area_a + area_b - area_overlap\n","    iou = area_overlap / (area_combined+epsilon)\n","    return iou\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### RCNN Model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:22:35.152265Z","iopub.status.busy":"2022-03-02T14:22:35.151935Z","iopub.status.idle":"2022-03-02T14:22:35.169547Z","shell.execute_reply":"2022-03-02T14:22:35.168454Z","shell.execute_reply.started":"2022-03-02T14:22:35.152231Z"},"trusted":true},"outputs":[],"source":["from torchvision.ops import RoIPool\n","\n","class FRCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        rawnet = torchvision.models.vgg16_bn(pretrained=True)\n","        for param in rawnet.features.parameters():\n","            param.requires_grad = True\n","        self.seq = nn.Sequential(*list(rawnet.features.children())[:-1])\n","        self.roipool = RoIPool(7, spatial_scale=14/224)\n","        feature_dim = 512*7*7\n","        self.cls_score = nn.Linear(feature_dim, len(label2target))\n","        self.bbox = nn.Sequential(\n","              nn.Linear(feature_dim, 512),\n","              nn.ReLU(),\n","              nn.Linear(512, 4),\n","              nn.Tanh(),\n","            )\n","        self.cel = nn.CrossEntropyLoss()\n","        self.sl1 = nn.L1Loss()\n","    def forward(self, input, rois, ridx):\n","        res = input\n","        res = self.seq(res)\n","        rois = torch.cat([ridx.unsqueeze(-1), rois*224], dim=-1)\n","        res = self.roipool(res, rois)\n","        feat = res.view(len(res), -1)\n","        cls_score = self.cls_score(feat)\n","        bbox = self.bbox(feat) # .view(-1, len(label2target), 4)\n","        return cls_score, bbox\n","    def calc_loss(self, probs, _deltas, labels, deltas):\n","        detection_loss = self.cel(probs, labels)\n","        ixs, = torch.where(labels != background_class)\n","        _deltas = _deltas[ixs]\n","        deltas = deltas[ixs]\n","        self.lmb = 10.0\n","        if len(ixs) > 0:\n","            regression_loss = self.sl1(_deltas, deltas)\n","            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss.detach()\n","        else:\n","            regression_loss = 0\n","            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:27:11.205321Z","iopub.status.busy":"2022-03-02T14:27:11.204099Z","iopub.status.idle":"2022-03-02T14:27:44.031242Z","shell.execute_reply":"2022-03-02T14:27:44.030370Z","shell.execute_reply.started":"2022-03-02T14:27:11.205268Z"},"trusted":true},"outputs":[],"source":["FPATHS, GTBBS, CLSS, DELTAS, ROIS, IOUS = [], [], [], [], [], []\n","N = 500\n","for ix, (im, bbs, labels, fpath) in enumerate(ds):\n","    if(ix==N):\n","        break\n","    H, W, _ = im.shape\n","    candidates = extract_candidates(im)\n","    candidates = np.array([(x,y,x+w,y+h) for x,y,w,h in candidates])\n","    ious, rois, clss, deltas = [], [], [], []\n","    ious = np.array([[extract_iou(candidate, _bb_) for candidate in candidates] for _bb_ in bbs]).T\n","    for jx, candidate in enumerate(candidates):\n","        cx,cy,cX,cY = candidate\n","        candidate_ious = ious[jx]\n","        best_iou_at = np.argmax(candidate_ious)\n","        best_iou = candidate_ious[best_iou_at]\n","        best_bb = _x,_y,_X,_Y = bbs[best_iou_at]\n","        if best_iou > 0.3: clss.append(labels[best_iou_at])\n","        else : clss.append('background')\n","        delta = np.array([_x-cx, _y-cy, _X-cX, _Y-cY]) / np.array([W,H,W,H])\n","        deltas.append(delta)\n","        rois.append(candidate / np.array([W,H,W,H]))\n","    FPATHS.append(fpath)\n","    IOUS.append(ious)\n","    ROIS.append(rois)\n","    CLSS.append(clss)\n","    DELTAS.append(deltas)\n","    GTBBS.append(bbs)\n","    \n","FPATHS = [f'{IMAGE_ROOT}/{stem(f)}.jpg' for f in FPATHS] \n","FPATHS, GTBBS, CLSS, DELTAS, ROIS = [item for item in [FPATHS, GTBBS, CLSS, DELTAS, ROIS]]"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:28:29.926712Z","iopub.status.busy":"2022-03-02T14:28:29.926350Z","iopub.status.idle":"2022-03-02T14:28:29.934365Z","shell.execute_reply":"2022-03-02T14:28:29.933763Z","shell.execute_reply.started":"2022-03-02T14:28:29.926665Z"},"trusted":true},"outputs":[],"source":["targets = pd.DataFrame(flatten(CLSS), columns=['label'])\n","label2target = {l:t for t,l in enumerate(targets['label'].unique())}\n","target2label = {t:l for l,t in label2target.items()}\n","background_class = label2target['background']"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:28:55.890210Z","iopub.status.busy":"2022-03-02T14:28:55.889351Z","iopub.status.idle":"2022-03-02T14:28:55.896779Z","shell.execute_reply":"2022-03-02T14:28:55.895866Z","shell.execute_reply.started":"2022-03-02T14:28:55.890169Z"},"trusted":true},"outputs":[],"source":["normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","def preprocess_image(img):\n","    img = torch.tensor(img).permute(2,0,1)\n","    img = normalize(img)\n","    return img.to(device).float()\n","def decode(_y):\n","    _, preds = _y.max(-1)\n","    return preds"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:34:01.621458Z","iopub.status.busy":"2022-03-02T14:34:01.621147Z","iopub.status.idle":"2022-03-02T14:34:01.636583Z","shell.execute_reply":"2022-03-02T14:34:01.635782Z","shell.execute_reply.started":"2022-03-02T14:34:01.621423Z"},"trusted":true},"outputs":[],"source":["class FRCNNDataset(Dataset):\n","    def __init__(self, fpaths, rois, labels, deltas, gtbbs):\n","        self.fpaths = fpaths\n","        self.gtbbs = gtbbs\n","        self.rois = rois\n","        self.labels = labels\n","        self.deltas = deltas\n","    def __len__(self): return len(self.fpaths)\n","    def __getitem__(self, ix):\n","        fpath = str(self.fpaths[ix])\n","        image = cv2.imread(fpath, 1)[...,::-1]\n","        gtbbs = self.gtbbs[ix]\n","        rois = self.rois[ix]\n","        labels = self.labels[ix]\n","        deltas = self.deltas[ix]\n","        assert len(rois) == len(labels) == len(deltas), f'{len(rois)}, {len(labels)}, {len(deltas)}'\n","        return image, rois, labels, deltas, gtbbs, fpath\n","\n","    def collate_fn(self, batch):\n","        input, rois, rixs, labels, deltas = [], [], [], [], []\n","        for ix in range(len(batch)):\n","            image, image_rois, image_labels, image_deltas, image_gt_bbs, image_fpath = batch[ix]\n","            image = cv2.resize(image, (224,224))\n","            input.append(preprocess_image(image/255.)[None])\n","            rois.extend(image_rois)\n","            rixs.extend([ix]*len(image_rois))\n","            labels.extend([label2target[c] for c in image_labels])\n","            deltas.extend(image_deltas)\n","        input = torch.cat(input).to(device)\n","        rois = torch.Tensor(rois).float().to(device)\n","        rixs = torch.Tensor(rixs).float().to(device)\n","        labels = torch.Tensor(labels).long().to(device)\n","        deltas = torch.Tensor(deltas).float().to(device)\n","        return input, rois, rixs, labels, deltas"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:34:13.576651Z","iopub.status.busy":"2022-03-02T14:34:13.575512Z","iopub.status.idle":"2022-03-02T14:34:13.586354Z","shell.execute_reply":"2022-03-02T14:34:13.585601Z","shell.execute_reply.started":"2022-03-02T14:34:13.576593Z"},"trusted":true},"outputs":[],"source":["n_train = 9*len(FPATHS)//10\n","train_ds = FRCNNDataset(FPATHS[:n_train], ROIS[:n_train], CLSS[:n_train], DELTAS[:n_train], GTBBS[:n_train])\n","test_ds = FRCNNDataset(FPATHS[n_train:], ROIS[n_train:], CLSS[n_train:], DELTAS[n_train:], GTBBS[n_train:])\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","train_loader = DataLoader(train_ds, batch_size=2, collate_fn=train_ds.collate_fn, drop_last=True)\n","test_loader = DataLoader(test_ds, batch_size=2, collate_fn=test_ds.collate_fn, drop_last=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:34:31.892298Z","iopub.status.busy":"2022-03-02T14:34:31.891827Z","iopub.status.idle":"2022-03-02T14:34:31.901296Z","shell.execute_reply":"2022-03-02T14:34:31.900458Z","shell.execute_reply.started":"2022-03-02T14:34:31.892245Z"},"trusted":true},"outputs":[],"source":["def train_batch(inputs, model, optimizer, criterion):\n","    input, rois, rixs, clss, deltas = inputs\n","    model.train()\n","    optimizer.zero_grad()\n","    _clss, _deltas = model(input, rois, rixs)\n","    loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n","    accs = clss == decode(_clss)\n","    loss.backward()\n","    optimizer.step()\n","    return loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()\n","def validate_batch(inputs, model, criterion):\n","    input, rois, rixs, clss, deltas = inputs\n","    with torch.no_grad():\n","        model.eval()\n","        _clss,_deltas = model(input, rois, rixs)\n","        loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n","        _clss = decode(_clss)\n","        accs = clss == _clss\n","    return _clss, _deltas, loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()"]},{"cell_type":"markdown","metadata":{},"source":["### Train"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:34:48.322109Z","iopub.status.busy":"2022-03-02T14:34:48.321815Z","iopub.status.idle":"2022-03-02T14:43:45.545804Z","shell.execute_reply":"2022-03-02T14:43:45.544649Z","shell.execute_reply.started":"2022-03-02T14:34:48.322078Z"},"trusted":true},"outputs":[],"source":["frcnn = FRCNN().to(device)\n","criterion = frcnn.calc_loss\n","optimizer = optim.SGD(frcnn.parameters(), lr=1e-3)\n","\n","n_epochs = 5\n","log = Report(n_epochs)\n","for epoch in range(n_epochs):\n","\n","    _n = len(train_loader)\n","    for ix, inputs in enumerate(train_loader):\n","        loss, loc_loss, regr_loss, accs = train_batch(inputs, frcnn, \n","                                                      optimizer, criterion)\n","        pos = (epoch + (ix+1)/_n)\n","        log.record(pos, trn_loss=loss.item(), trn_loc_loss=loc_loss, \n","                   trn_regr_loss=regr_loss, \n","                   trn_acc=accs.mean(), end='\\r')\n","        \n","    _n = len(test_loader)\n","    for ix,inputs in enumerate(test_loader):\n","        _clss, _deltas, loss, \\\n","        loc_loss, regr_loss, accs = validate_batch(inputs, \n","                                                frcnn, criterion)\n","        pos = (epoch + (ix+1)/_n)\n","        log.record(pos, val_loss=loss.item(), val_loc_loss=loc_loss, \n","                val_regr_loss=regr_loss, \n","                val_acc=accs.mean(), end='\\r')\n","        \n","    log.report_avgs(epoch+1)\n","\n","# Plotting training and validation metrics\n","log.plot_epochs('trn_loss,val_loss'.split(','))"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:48:44.559152Z","iopub.status.busy":"2022-03-02T14:48:44.558848Z","iopub.status.idle":"2022-03-02T14:48:44.855160Z","shell.execute_reply":"2022-03-02T14:48:44.854343Z","shell.execute_reply.started":"2022-03-02T14:48:44.559118Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import matplotlib.patches as mpatches\n","from torchvision.ops import nms\n","from PIL import Image\n","def test_predictions(filename):\n","    img = cv2.resize(np.array(Image.open(filename)), (224,224))\n","    candidates = extract_candidates(img)\n","    candidates = [(x,y,x+w,y+h) for x,y,w,h in candidates]\n","    input = preprocess_image(img/255.)[None]\n","    rois = [[x/224,y/224,X/224,Y/224] for x,y,X,Y in candidates]\n","    rixs = np.array([0]*len(rois))\n","    rois, rixs = [torch.Tensor(item).to(device) for item in [rois, rixs]]\n","    with torch.no_grad():\n","        frcnn.eval()\n","        probs, deltas = frcnn(input, rois, rixs)\n","        confs, clss = torch.max(probs, -1)\n","    candidates = np.array(candidates)\n","    confs, clss, probs, deltas = [tensor.detach().cpu().numpy() for tensor in [confs, clss, probs, deltas]]\n","    \n","    ixs = clss!=background_class\n","    confs, clss, probs, deltas, candidates = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates]]\n","    bbs = candidates + deltas\n","    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n","    confs, clss, probs, deltas, candidates, bbs = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n","    if len(ixs) == 1:\n","        confs, clss, probs, deltas, candidates, bbs = [tensor[None] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n","    \n","    bbs = bbs.astype(np.uint16)\n","    _, ax = plt.subplots(1, 2, figsize=(20,10))\n","    show(img, ax=ax[0])\n","    ax[0].grid(False)\n","    ax[0].set_title(filename.split('/')[-1])\n","    if len(confs) == 0:\n","        ax[1].imshow(img)\n","        ax[1].set_title('No objects')\n","        plt.show()\n","        return\n","    else:\n","        show(img, bbs=bbs.tolist(), texts=[target2label[c] for c in clss.tolist()], ax=ax[1])\n","        plt.show()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T14:49:20.909484Z","iopub.status.busy":"2022-03-02T14:49:20.909190Z","iopub.status.idle":"2022-03-02T14:49:21.785132Z","shell.execute_reply":"2022-03-02T14:49:21.784524Z","shell.execute_reply.started":"2022-03-02T14:49:20.909450Z"},"trusted":true},"outputs":[],"source":["test_predictions(ds[30][-1])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
